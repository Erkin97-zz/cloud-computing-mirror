a b c	2
a backup namenode	1
a backup the	1
a block protocol	1
a bottleneck for	1
a certain extent	1
a client in	1
a cluster it	1
a cluster of	1
a collection of	1
a columnoriented database	1
a contains data	1
a data node	2
a data store	1
a database that	1
a datanode and	1
a dedicated namenode	1
a default pool	1
a different rack	1
a distributed filesystem	1
a distributed scalable	1
a distributed storage	1
a failed primary	1
a file url	1
a filesystem in	1
a fraction of	1
a full randomaccess	1
a fully posixcompliant	1
a fundamental assumption	1
a guaranteed minimum	1
a hadoop application	2
a hadoop cluster	1
a hadoop file	1
a hadoop is	1
a heartbeat from	1
a heartbeat is	1
a heartbeat message	1
a high level	1
a highperformance distributed	1
a huge number	1
a job is	1
a job tracker	1
a job with	1
a large number	1
a larger cluster	1
a limit on	1
a linux cluster	1
a locationaware ibrix	1
a mapreduce engine	1
a mapreduce programming	1
a master node	1
a misleading term	1
a more conventional	1
a multinode hadoop	1
a network of	1
a new abstraction	1
a new addition	1
a number of	4
a parallel file	1
a pb per	1
a platform responsible	1
a posix filesystem	1
a price the	1
a processing part	1
a queue a	1
a rack power	1
a rackaware file	1
a realtime system	1
a secondary namenode	1
a separate java	1
a significant impact	1
a single master	1
a single namenode	1
a single task	1
a slave or	1
a small hadoop	1
a socalled secondary	1
a software framework	1
a standalone jobtracker	1
a storage part	1
a table and	1
a tasktracker fails	1
a to perform	1
a traditional onsite	1
a variety of	1
a very substantial	1
a web browser	1
a work queue	1
a worker node	1
ability to use	1
about lines of	2
about the location	1
abstraction for information	1
abstractions a mapreduce	1
accept all data	1
access can be	1
access data in	1
access from hadoop	1
access that evolve	1
access to the	1
access to this	1
accessible ftp servers	1
according to its	1
accumulo secure bigtable	1
achieved through the	1
achieves reliability by	1
acquire hardware or	1
across multiple data	1
across multiple hosts	1
across multiple machines	1
across multiple racks	1
across nodes in	1
across nodes when	1
across the cluster	1
across various hardware	1
actions then to	1
active map or	1
acts as both	1
actual data into	1
actual node where	1
add to the	1
added to hdfs	1
adding the ability	1
addition aims to	1
addition of yarn	1
additional software packages	1
adoption had become	1
advantage is not	1
advantage of data	1
advantage of using	1
after his sons	2
against the parascale	1
aggregate bandwidth across	1
aims to tackle	1
al bigtable a	1
al hstore a	1
algorithms on a	1
alive in this	1
all data formats	1
all its data	1
all the modules	1
allocate resources to	1
allocated a fraction	1
allocated machine and	1
allocated to queues	1
allocates work to	1
allocation of work	1
allocation to applications	1
allowing multiple namespaces	1
allows distributions of	1
allows organizations to	1
allows the dataset	1
alon halevy david	1
alongside hadoop such	1
also be used	1
also been written	1
also found use	1
also hadoop distributed	1
also hadoop permits	1
also it receives	1
also known as	2
also started developing	1
also the ecosystem	1
alternate file system	1
alternate scheduler such	1
alternative file system	2
although redundancy options	1
always available this	1
amazon elastic compute	1
amazon s simple	1
amount of traffic	1
amounts of data	1
an alternate file	1
an alternate scheduler	1
an alternative file	2
an available slot	1
an awareness of	1
an extension of	1
an implementation of	1
an increasingly important	1
an opensource implementation	1
an uptodate directory	1
analysis of various	1
and a processing	1
and a secondary	1
and about lines	1
and also it	1
and also the	1
and apache storm	1
and apply on	1
and benefits from	1
and big data	1
and bottlenecks in	1
and bottom two	1
and builds snapshots	1
and can be	1
and can talk	1
and command line	1
and computation it	1
and computeonly worker	1
and conveys that	1
and data are	1
and data node	1
and datanode a	1
and datanode architecture	1
and derivative works	1
and distributes them	1
and failing that	1
and for compatibility	1
and google file	1
and growth of	1
and hadoop is	1
and hadoop yarn	1
and has the	1
and hdfs components	1
and hence its	1
and hence theoretically	1
and in the	1
and information is	1
and it stores	1
and it will	1
and java application	1
and later that	1
and loss of	1
and management of	1
and mapreduce hdfs	1
and mapreduce is	1
and may not	1
and mike cafarella	1
and more efficiently	1
and multiple worker	1
and no hdfs	1
and node x	2
and ocaml the	1
and one on	1
and opensource software	1
and operating system	1
and optionally scheduling	1
and other details	1
and portable file	1
and prevents unnecessary	1
and processing of	1
and produced data	1
and quality of	1
and reduce parts	1
and regular http	1
and resource allocation	1
and runs on	1
and scripts needed	1
and should be	1
and shutdown scripts	1
and some other	1
and spark streaming	1
and starts the	1
and storage management	1
and submodules and	1
and support for	1
and task tracker	1
and tasktracker status	1
and tasktracker the	1
and tasktracker though	1
and the apache	1
and the hadoop	1
and the term	1
and the underlying	1
and the whole	1
and to keep	1
and to provide	1
and using them	1
and utilities needed	1
and where the	1
and which other	1
and write these	1
andor clickstream analysis	1
andor sophisticated data	1
andor text processing	1
announced that the	1
announced the availability	1
announced the data	1
another one from	1
another resource negotiator	1
any distributed file	1
any of these	1
any programming language	1
any sort of	1
apache accumulo secure	1
apache cassandra a	1
apache couchdb a	1
apache flume apache	1
apache hadoop consists	1
apache hadoop framework	1
apache hadoop hdup	1
apache hadoop or	1
apache hadoop project	1
apache hadoop the	1
apache hadoop were	1
apache hadoops mapreduce	1
apache hbase and	1
apache hbase apache	1
apache hcatalog a	1
apache hive apache	1
apache hive data	1
apache mahout machine	1
apache nutch project	1
apache oozie and	1
apache phoenix apache	1
apache pig apache	1
apache software foundation	1
apache spark apache	1
apache sqoop apache	1
apache storm flink	1
apache the list	1
apache zookeeper cloudera	1
api generates a	1
api methods that	1
api the thrift	1
apis for data	1
appistry released a	1
application master which	1
application over http	1
application programming interface	1
application that runs	1
application the tradeoff	1
application the yahoo	1
applications can use	1
applications effectively it	1
applications many of	1
applications of hadoop	1
applications submit mapreduce	1
applications the application	1
apply on the	1
applying that code	1
approach reduces the	1
approach takes advantage	1
april appistry released	1
april it continues	1
april parascale published	1
architecture apache storm	1
architecture of hdfs	1
architecture that relies	1
archive jar files	1
archiving including of	1
are allocated a	1
are allocated to	1
are available for	1
are being made	1
are closest to	1
are common occurrences	1
are containers working	1
are currently in	1
are currently several	1
are designed with	1
are distributed via	1
are exclusive to	1
are grouped into	1
are important features	1
are managed through	1
are master servicesdaemonsnodes	1
are multiple hadoop	1
are nearby if	1
are normally used	1
are replaced by	1
are similar to	2
are slave daemons	1
are slave services	1
are some issues	1
are somewhat controversial	1
are split across	1
are still useful	1
are stored and	1
are uncategorized go	1
are under development	1
around and to	1
array of independent	1
as a backup	1
as a limit	1
as apache pig	1
as both a	1
as close to	1
as dead and	1
as demonstrated with	1
as hadoop distributed	1
as in the	1
as it is	1
as lambda architecture	1
as master node	1
as of hadoop	1
as of october	1
as possible with	1
as shell scripts	1
as single point	1
as slots every	1
as small file	1
as the blocks	1
as the checkpoint	1
as the default	1
as the fair	1
as the slave	1
as we have	1
as well as	2
assigned a guaranteed	1
assumption that hardware	1
at a price	1
at apache the	1
at clusters hosted	1
at scale has	1
at the time	1
at what data	1
at yahoo and	1
at yahoo at	1
atop the file	1
authors highlight the	1
automatically handled by	1
availability of an	1
available for the	1
available slot there	1
available slots such	1
available tasktracker nodes	1
available this can	1
available to the	1
awareness between the	1
awareness of the	1
awareness which is	1
azure blob stores	1
azure storage blobs	1
b c and	2
backbone network if	1
backbone traffic hdfs	1
backup namenode when	1
backup the project	1
bandwidth across the	1
base apache hadoop	1
base modules and	1
based on the	1
basically job tracker	1
batchoriented rather than	1
be a data	1
be achieved through	1
be automatically handled	1
be called apache	1
be deployed in	1
be executed on	1
be hosted on	1
be in a	1
be installed on	1
be mounted by	1
be mounted directly	1
be particular name	1
be processed faster	1
be scheduled to	1
be set up	1
be suitable for	1
be used for	2
be used to	2
be used with	1
be useful in	1
be viewed from	1
because the namenode	1
because the requirements	1
become a bottleneck	1
become an increasingly	1
become widespread more	1
been written none	1
being made to	1
benefit to execute	1
benefits from parallel	1
between hadoop and	3
between nodes in	1
between the job	1
beyond their total	1
big data processing	1
big data using	1
biggest changes is	1
biggest difference between	1
bigtable a distributed	1
birth and growth	1
blob stores without	1
blobs wasb file	1
block protocol specific	1
block replications on	1
blocks and distributes	1
blocks locations at	1
blocks of data	1
blocks this is	1
bootstraps the linux	1
both a datanode	1
both base modules	1
bottleneck for supporting	1
bottlenecks in huge	1
bottlenecks since the	1
bottom two are	1
bridges can provide	1
bridges have also	1
builds snapshots of	1
built from commodity	1
bundled with apache	1
but it does	1
but to increase	1
but was moved	1
by allowing multiple	1
by default hadoop	1
by default jobs	1
by facebook the	1
by google papers	1
by hadoop for	1
by jetty and	1
by other hadoop	1
by replicating the	1
by roughly half	1
by separate namenodes	1
by simply using	1
by the apache	1
by the filesystemspecific	1
by the framework	1
by the underlying	1
by yahoo the	1
c and command	1
c and node	2
c cocoa smalltalk	1
c java python	1
cafarella the genesis	1
calculations for the	1
call it as	2
called apache hadoop	1
called the namenode	1
calls rpc to	1
can also be	1
can be achieved	1
can be called	1
can be deployed	1
can be executed	1
can be installed	1
can be mounted	2
can be used	3
can be viewed	1
can become a	1
can communicate with	2
can delay the	1
can end up	1
can generate snapshots	1
can have a	1
can in theory	1
can manage job	1
can talk to	1
can talk with	1
can track the	1
can use this	1
cannot be hosted	1
cannot use features	1
capabilities were added	1
capacity is split	1
capacity scheduler described	1
capacity scheduler supports	1
capacity scheduler was	1
care of the	1
care of two	1
cassandra a columnoriented	1
centers every hadoop	1
certain extent by	1
chang et al	1
changes is that	1
check its status	1
checkpoint node it	1
checkpointed images can	1
checkpoints of the	1
claimed that they	1
claimed was the	1
clickstream analysis of	1
client applications submit	1
client in a	1
client job tracker	1
client to read	1
clients use remote	1
close to the	1
closest to the	1
cloud allows organizations	1
cloud serverondemand infrastructure	1
cloud the cloud	1
cloudera and datadog	1
cloudera impala apache	1
cloudiq storage product	1
cluster has nominally	1
cluster hdfs nodes	1
cluster in the	1
cluster includes a	1
cluster it then	1
cluster node bootstraps	1
cluster of datanodes	1
cluster striving to	1
cluster which is	1
cluster with more	1
clusters and using	1
clusters at yahoo	1
clusters built from	1
clusters development started	1
clusters google this	1
clusters hosted on	1
clusters of higherend	1
clusters perform is	1
cocoa smalltalk and	1
code and apply	1
code for hdfs	1
code for mapreduce	1
code from the	1
code in c	1
code into nodes	1
code is common	1
code of its	1
code on the	2
code that was	1
code to run	1
code was published	1
cofounders doug cutting	1
collection of additional	1
collection of opensource	1
columnoriented database that	1
comes at a	1
comes the mapreduce	1
command line utilities	1
commandline interface the	1
commands and java	1
commercial applications of	1
commercial distributions of	1
commercial implementations or	1
committer to add	1
commodity hardwarestill the	1
commodity machines providing	1
common any programming	1
common contains libraries	1
common occurrences and	1
common package contains	1
common package which	1
common useit has	1
communicate with each	3
communication clients use	1
companies offer commercial	1
companies used hadoop	1
compatibility with a	1
compatible are somewhat	1
complement a realtime	1
compliance but it	1
components were inspired	1
composed of the	1
computation and data	1
computation it provides	1
compute cloud serverondemand	1
computeonly worker nodes	1
computer clusters built	1
computers to solve	1
computing resources in	1
concurrent write operations	1
configurations are still	1
connect with the	1
connects with the	1
consider it to	1
consideration of the	1
consisted of about	1
consists of a	2
consists of one	1
consists of only	1
consists of the	1
containers working in	1
contains data a	1
contains data x	1
contains libraries and	1
contains the data	1
contains the details	1
contains the java	1
continues to evolve	1
contributions that are	1
controversial within the	1
conventional supercomputer architecture	1
conveys that it	1
copies around and	1
core of apache	1
cores and produced	1
corresponding slave node	1
corruption and loss	1
couchdb a database	1
crashes its jvm	1
crawling andor text	1
create an uptodate	1
criticality each datanode	1
current system load	1
currently in hadoop	1
currently several monitoring	1
cutting and mike	1
cutting to develop	1
cutting who was	1
daemons every data	1
data a b	1
data access that	1
data across multiple	1
data and computation	1
data and mapreduce	1
data and the	1
data and which	1
data are distributed	1
data as possible	1
data awareness between	1
data centers every	1
data eg for	1
data for data	1
data formats and	1
data had grown	1
data hdfs has	1
data high hdfs	1
data in azure	1
data in it	2
data in parallel	1
data information that	1
data intensive computing	1
data into hdfs	1
data is and	1
data is stored	2
data it can	1
data job tracker	1
data like job	1
data locality where	1
data location for	1
data name node	1
data node a	1
data node as	1
data node for	1
data node is	1
data node sends	1
data node stores	1
data node the	1
data nodes can	1
data on commodity	1
data on remotely	1
data over the	1
data permanently into	1
data processing on	2
data processing some	1
data redundancy across	1
data resides priority	1
data similarly a	1
data store due	1
data that was	1
data they have	1
data throughput and	1
data to job	1
data to move	1
data transfer when	1
data using the	1
data warehouse system	1
data was growing	1
data will remain	1
data with an	1
data x y	1
database that supports	1
database that uses	1
database the apache	1
databases to dataspaces	1
datacenter as well	1
dataintensive and benefits	1
datanode a slave	1
datanode and tasktracker	1
datanode architecture of	1
datanode serves up	1
datanodes although redundancy	1
datanodes namenodes and	1
dataonly and computeonly	1
dataset to be	1
dataspaces a new	1
david maier from	1
dead and starts	1
dean sanjay ghemawat	1
decreases storage overhead	1
dedicated namenode server	1
deep learning algorithms	1
default hadoop uses	1
default jobs that	1
default pool pools	1
default replication value	1
default specifically ibm	1
delay the entire	1
demonstrated with dataintensive	1
deploy hadoop without	1
deployed in a	1
derivative works from	1
design introduces portability	1
designed for computer	1
designed for mostly	1
designed for portability	1
designed to scale	1
designed with a	1
details as we	1
details of the	1
develop an opensource	1
developed by facebook	1
developed by yahoo	1
developing automatic failovers	1
development at apache	1
development started on	1
differ from the	1
difference between hadoop	3
different rack data	1
different tasks the	1
direct connect with	1
directly with a	1
directly with any	1
directories these checkpointed	1
directory information which	1
directory structure because	1
discussed a locationaware	1
discussed running hadoop	1
disks raid storage	1
distributed file system	7
distributed filesystem that	1
distributed main memory	1
distributed scalable and	1
distributed storage and	2
distributed storage system	1
distributed via highspeed	1
distributes them across	1
distribution work that	1
distributions however some	1
distributions of apache	1
distributions of hadoop	2
divided into hdfs	1
dmons which take	1
docker which reduces	1
documents javascript for	1
does job tracking	1
does not receive	1
does not require	1
does provide shell	1
doug cutting and	1
doug cutting to	1
doug cutting who	1
driver for use	1
due to its	3
each datanode serves	1
each node spawns	1
each other and	1
each other name	1
each other to	1
each pool is	1
ecosystem expose richer	1
ecosystem or collection	1
edit the log	1
effective scheduling of	1
effectively it runs	1
efficiently than it	1
eg c java	1
eg for compliance	1
either mapreducemr or	1
elastic compute cloud	1
elephant the initial	1
enabled however a	1
enables having multiple	1
end up waiting	1
end when everything	1
endtoend performance requires	1
engine either mapreducemr	1
engine in june	1
engine in the	1
engine which consists	1
enterpriselevel infrastructure monitoring	1
entire journal of	1
entire mapreduce job	1
environment jre or	1
erlang perl haskell	1
especially a large	1
especially towards the	1
et al bigtable	1
et al hstore	1
every active map	1
every data node	1
every few minutes	1
every hadoop cluster	1
every hadoopcompatible file	1
every seconds and	1
every tasktracker has	1
every yahoo web	1
everything can end	1
evolve based on	1
evolve through contributions	1
example if node	1
example while there	1
excess capacity is	1
exclusive to the	1
execute code on	1
execute deep learning	1
executed on multiple	1
execution enabled however	1
execution from the	1
expose richer user	1
exposed by jetty	1
extension of hdfs	1
extent by allowing	1
facebook claimed that	1
facebook the goal	1
facilitate using a	1
fact the secondary	1
factored out of	1
failed primary namenode	1
failing if the	1
failing that on	1
failover onto a	1
fails or times	1
failure if any	1
failure it has	1
failure spof and	1
failures are common	1
failures occurs the	1
fair scheduler has	1
fair scheduler is	1
fair scheduler or	1
fair scheduler was	1
fast response times	1
faster and more	1
fay chang et	1
features provided by	1
features that are	2
february yahoo inc	1
federation a new	1
few minutes to	1
fifo scheduling and	1
file access can	1
file is known	1
file issues scalability	1
file system and	2
file system apache	1
file system as	2
file system bridges	2
file system driver	2
file system for	1
file system hdfs	4
file system includes	1
file system index	1
file system is	1
file system metadata	1
file system on	1
file system paper	1
file system should	1
file system that	1
file system the	3
file system this	3
file system uses	1
file system where	1
file system with	1
file system written	1
file systems a	1
file systems bundled	1
file systems comes	1
file systems of	1
file systems or	1
file systems this	1
file the process	1
file url however	1
files and may	1
files and scripts	1
files especially a	1
files hdfs federation	1
files into large	1
files manage the	1
files typically in	1
filesystem actions then	1
filesystem corruption and	1
filesystem differ from	1
filesystem in userspace	1
filesystem is increased	1
filesystem that stores	1
first committer to	1
first version of	1
five services as	1
flink and spark	1
flume apache sqoop	1
for a posix	1
for an api	1
for any sort	1
for both base	1
for communication clients	1
for compatibility with	1
for computer clusters	1
for data access	1
for data redundancy	1
for data throughput	1
for distributed storage	1
for documents javascript	1
for effective scheduling	1
for example if	1
for example while	1
for hadoop mapr	1
for hdfs and	1
for information management	1
for largescale data	1
for managing computing	1
for map reduce	1
for mapreduce queries	1
for minutes it	1
for mostly immutable	1
for nonposix operations	1
for other applications	1
for portability across	1
for production jobs	1
for resource management	1
for scheduling users	1
for small jobs	1
for storage and	1
for storage systems	1
for storing the	1
for structured data	1
for supporting a	1
for systems requiring	1
for the client	1
for the hadoop	1
for the job	1
for the name	1
for the namenode	1
for the processing	2
for the slowest	1
for the yahoo	1
for use with	1
formats and to	1
fortune companies used	1
found use on	1
foundation has stated	1
fraction of the	1
framework for distributed	1
framework he named	1
framework is composed	1
framework itself is	1
framework some consider	1
franklin alon halevy	1
free and opensource	1
free resources are	1
from a data	1
from a web	1
from a work	1
from commodity hardwarestill	1
from databases to	1
from datanodes namenodes	1
from failing if	1
from google mapreduce	1
from other vendors	1
from parallel processing	1
from the client	1
from the job	2
from the target	1
from the tasktracker	1
fs which replaced	1
ftp file system	1
full randomaccess readwrite	1
fully posixcompliant because	1
fully posixcompliant filesystem	1
fundamental assumption that	1
fuse virtual file	1
fusion file system	1
general archiving including	1
general parallel file	1
generate snapshots of	1
generates a client	1
genesis of hadoop	1
ghemawat mapreduce simplified	1
gigabytes to terabytes	1
given to nodes	1
gives the meta	1
go into a	1
goal of the	1
goals of a	1
goes offline in	1
goes over the	1
google file system	2
google mapreduce simplified	1
google papers on	1
google this paper	1
gpu hardware within	1
grouped into pools	1
growing by roughly	1
grown to pb	1
growth of hadoop	1
guaranteed minimum share	1
had become widespread	1
had grown to	1
had the largest	1
hadoop adoption had	1
hadoop after his	1
hadoop against the	1
hadoop and big	1
hadoop and hadoop	3
hadoop application that	1
hadoop application the	1
hadoop applications can	1
hadoop are designed	1
hadoop can be	1
hadoop can in	1
hadoop cluster has	1
hadoop cluster in	1
hadoop cluster includes	1
hadoop cluster node	1
hadoop clusters at	1
hadoop common contains	1
hadoop common package	2
hadoop consists of	2
hadoop decreases storage	1
hadoop developer community	1
hadoop distributed file	6
hadoop distribution work	1
hadoop distributions however	1
hadoop ecosystem expose	1
hadoop enables having	1
hadoop file system	1
hadoop for example	1
hadoop framework is	1
hadoop framework itself	1
hadoop framework some	1
hadoop hadoop enables	1
hadoop hdup is	1
hadoop hosting in	1
hadoop is divided	1
hadoop is often	1
hadoop is the	1
hadoop is used	1
hadoop mapr fs	1
hadoop mapreduce an	1
hadoop mapreduce is	1
hadoop needs to	1
hadoop or distributions	1
hadoop over the	1
hadoop permits usage	1
hadoop production application	1
hadoop project can	1
hadoop project hadoop	1
hadoop requires java	1
hadoop ship with	1
hadoop splits files	1
hadoop streaming to	1
hadoop subproject in	1
hadoop such as	1
hadoop the naming	1
hadoop there are	1
hadoop to access	1
hadoop uses fifo	1
hadoop version available	1
hadoop was released	1
hadoop was the	1
hadoop without the	1
hadoop works directly	1
hadoop yarn introduced	1
hadoop yarn strives	1
hadoopcompatible file system	1
hadoops mapreduce and	1
hadoops own rackaware	1
hadoopspecific file system	1
halevy david maier	1
half a pb	1
half of the	1
handled by the	1
hardware all the	1
hardware failures are	1
hardware failures occurs	1
hardware or specific	1
hardware platforms and	1
hardware within the	1
hardwarestill the common	1
has a number	1
has access to	1
has also found	1
has also started	1
has become an	1
has direct connect	1
has five services	1
has nominally a	1
has stated that	1
has the meta	1
has three basic	1
haskell c cocoa	1
have a significant	1
have access to	1
have also been	1
have dataonly and	1
have only one	1
have to specify	1
having a fully	1
having multiple name	1
having to replay	1
hbase and mapreduce	1
hbase apache phoenix	1
hbase database the	1
hcatalog a table	1
hdfs a distributed	1
hdfs and a	1
hdfs and about	1
hdfs and mapreduce	1
hdfs are replaced	1
hdfs can be	1
hdfs components were	1
hdfs consists of	1
hdfs design introduces	1
hdfs federation a	1
hdfs file system	3
hdfs file systems	1
hdfs hadoops own	1
hdfs has five	1
hdfs is a	1
hdfs is data	1
hdfs is designed	1
hdfs is not	1
hdfs is running	1
hdfs is used	1
hdfs letting the	1
hdfs nodes are	1
hdfs performance at	1
hdfs performance including	1
hdfs stores large	1
hdfs such as	1
hdfs that allows	1
hdfs the file	1
hdfs the hadoop	1
hdfs uses this	1
hdfs was designed	1
hdfs which is	1
hdfsui web application	1
hdup is a	1
he named it	1
heartbeat from a	1
heartbeat is sent	1
heartbeat message to	1
helper node for	1
hence its actual	1
hence theoretically does	1
high aggregate bandwidth	1
high hdfs is	1
high level of	1
high performance computing	1
highavailability capabilities were	1
higher the standard	1
higherend hardware all	1
highlight the need	1
highperformance distributed main	1
his sons toy	2
hive apache hbase	1
hive data warehouse	1
hortonworks cloudera and	1
host the file	1
hosted on the	2
hosting in the	1
hosts and hence	1
hosts but to	1
however a single	1
however some commercial	1
however this comes	1
hp discussed a	1
hpcc lexisnexis risk	1
hstore a highperformance	1
http for an	1
http or via	1
huge metadata requests	1
huge number of	1
hypertable hbase alternative	1
ibm and mapr	1
ibm discussed running	1
ibm general parallel	1
ibrix fusion file	1
if a tasktracker	1
if any of	1
if node a	1
if one tasktracker	1
if the running	1
if the work	1
image including the	1
images can be	1
immutable files and	1
impact of a	1
impact on jobcompletion	1
impala apache flume	1
implement the map	1
implementation cannot use	1
implementation of the	2
implementations or support	1
important features provided	1
important issue monitoring	1
in a cluster	1
in a larger	1
in a more	1
in a number	1
in a platform	1
in a traditional	1
in april appistry	1
in april it	1
in april parascale	1
in azure blob	1
in c and	1
in clusters and	1
in every yahoo	1
in facebook claimed	1
in fact the	1
in hadoop are	1
in hadoop distributions	1
in hadoop hadoop	1
in hadoop there	1
in hdfs such	1
in huge metadata	1
in ibm discussed	1
in it as	1
in it to	1
in january doug	1
in java for	1
in june hp	1
in june they	1
in june yahoo	1
in march owen	1
in may highavailability	1
in may mapr	1
in may the	1
in nonstandard applications	1
in october this	1
in parallel this	1
in principle of	1
in response gives	1
in some performance	1
in the cloud	2
in the cluster	2
in the first	1
in the hadoop	1
in the java	1
in the name	1
in the processing	1
in the range	1
in the same	2
in the world	1
in theory be	1
in this file	1
in this way	1
in userspace fuse	1
in version the	1
inc announced the	1
inc launched what	1
include the index	1
includes a single	1
includes a socalled	1
includes the hbase	1
including hortonworks cloudera	1
including of relationaltabular	1
including the hadoop	1
incorrectly interpret as	1
increase inputoutput io	1
increased performance for	1
increasingly important issue	1
independent disks raid	1
index and a	1
index calculations for	1
influenced the birth	1
information is exposed	1
information management the	1
information that hadoopspecific	1
information to execute	1
information which the	1
infrastructure monitoring hdfs	1
infrastructure there is	1
initial code that	1
inputoutput io performance	1
inspired by google	1
inspired doug cutting	1
installed on top	1
instead be a	1
integration into enterpriselevel	1
interface api methods	1
interface the hdfsui	1
interpret as a	1
into a default	1
into enterpriselevel infrastructure	1
into hdfs and	1
into hdfs which	1
into large blocks	1
into nodes to	1
into the cluster	1
introduced in a	1
introduces portability limitations	1
involving massive amounts	1
io performance some	1
is a collection	1
is a distributed	1
is a hadoop	1
is a mapreduce	1
is a master	1
is a very	1
is alive in	1
is all remote	1
is also known	2
is an extension	1
is and failing	1
is assigned a	1
is batchoriented rather	1
is common any	1
is composed of	1
is data awareness	1
is designed for	1
is designed to	1
is divided into	1
is exposed by	1
is given to	1
is hadoop applications	1
is helper node	1
is in the	1
is increased performance	1
is its corresponding	1
is known as	1
is known to	1
is mostly written	1
is no consideration	1
is no preemption	1
is no rackawareness	1
is not always	1
is not fully	1
is not restricted	1
is often used	1
is one single	1
is only to	1
is possible to	1
is rescheduled the	1
is responsible for	1
is running due	1
is sent from	1
is split between	1
is stored and	1
is stored on	1
is targeted at	1
is that hadoop	1
is the addition	1
is the name	1
is the single	1
is the slave	1
is to provide	1
is used for	2
is used with	2
is very dataintensive	1
is very simple	1
is very slow	1
issue monitoring endtoend	1
issues in hdfs	1
issues scalability problems	1
it achieves reliability	1
it after his	1
it as master	1
it as single	1
it as the	1
it can also	1
it can be	1
it can become	1
it can delay	1
it continues to	1
it does provide	1
it hadoop after	1
it has direct	1
it is alive	1
it is all	1
it is helper	1
it is possible	1
it is the	1
it provides a	1
it receives code	1
it runs two	1
it stores the	1
it then transfers	1
it to be	1
it to instead	1
it will take	2
it would be	1
its actual availability	1
its cofounders doug	1
its corresponding slave	1
its criticality each	1
its data on	1
its hadoop version	1
its jvm a	1
its lack of	1
its own cloudiq	1
its status the	1
its widespread integration	1
itself from failing	1
itself is mostly	1
january doug cutting	1
jar files and	1
java api the	1
java application programming	1
java archive jar	1
java code is	1
java for the	1
java implementation cannot	1
java programming language	1
java python php	1
java runtime environment	1
java virtual machine	1
javascript for mapreduce	1
jeffrey dean sanjay	1
jetty and can	1
job crashes its	1
job especially towards	1
job is rescheduled	1
job is running	1
job scheduler was	1
job scheduling across	1
job tracker allocates	1
job tracker and	4
job tracker basically	1
job tracker receives	1
job tracker schedules	2
job tracker talks	1
job tracker task	2
job tracker will	2
job tracking and	1
job with a	1
jobcompletion times as	1
jobs and quality	1
jobs are grouped	1
jobs are split	1
jobs from a	1
jobs it can	1
jobs that are	1
jobs the fair	1
jobs the jobtracker	1
jobs to task	1
jobtracker and tasktracker	1
jobtracker every few	1
jobtracker knows which	1
jobtracker pushes work	1
jobtracker server can	1
jobtracker to which	1
jobtracker while adding	1
journal of filesystem	1
jre or higher	1
json for documents	1
june hp discussed	1
june they announced	1
june yahoo made	1
jvm a heartbeat	1
jvm process to	1
kallman et al	1
keep the replication	1
keep the work	1
know about the	1
know which servers	1
known as hadoop	1
known as mapper	1
known as the	2
known limitations of	1
known to include	1
knows which node	1
lack of posix	1
lambda architecture apache	1
language can be	1
language with some	1
languages eg c	1
large blocks and	1
large clusters development	1
large clusters google	1
large files typically	1
large number of	1
larger cluster hdfs	1
largescale data processing	1
largest hadoop cluster	1
largest hadoop production	1
later that year	1
launched what they	1
layer for hadoop	1
learning algorithms on	1
learning andor sophisticated	1
learning system and	1
letting the main	1
level abstractions a	1
level of priority	1
lexisnexis risk solutions	1
libraries and utilities	1
like job tracker	1
limit on the	1
limitations of this	1
limitations that result	1
line utilities written	1
lines of code	2
linux and some	1
linux cluster with	1
linux image including	1
linux utility for	1
list includes the	1
list of supported	1
load of the	1
local or remote	1
locality to reduce	1
locality where nodes	1
location awareness which	1
location for example	1
location of the	1
locationaware ibrix fusion	1
locations at what	1
log andor clickstream	1
log to create	1
loss of data	1
loss of locality	1
machine and hence	1
machine jvm process	1
machine learning andor	1
machine learning system	1
machines are nearby	1
machines it achieves	1
machines providing very	1
made the source	1
made to the	1
mahout machine learning	1
maier from databases	1
main article mapreduce	1
main backbone network	1
main memory transaction	1
main metadata server	1
manage job scheduling	1
manage the file	1
managed through a	1
management layer for	1
management of metadata	1
management the authors	1
manager which does	1
managing computing resources	1
manipulate the data	1
manually failover onto	1
many computers to	1
many of which	1
map and reduce	1
map or reduce	4
map reduce execution	1
map slots reduce	1
mapr fs which	1
mapr technologies inc	1
mapreduce an implementation	1
mapreduce and google	1
mapreduce and hdfs	1
mapreduce engine either	1
mapreduce engine in	1
mapreduce engine which	1
mapreduce framework he	1
mapreduce hdfs is	1
mapreduce is used	2
mapreduce java code	1
mapreduce job especially	1
mapreduce jobs are	1
mapreduce jobs it	1
mapreduce jobs the	1
mapreduce programming model	3
mapreduce queries and	1
mapreduce simplified data	2
mapreducemr or yarnmr	1
march owen omalley	1
massive amounts of	1
master and multiple	1
master node and	1
master node consists	1
master node which	1
master services can	1
master servicesdaemonsnodes and	1
master which monitors	1
may highavailability capabilities	1
may mapr technologies	1
may not be	1
may the list	1
memory structures thereby	1
memory transaction processing	1
message to the	1
meta data and	1
meta data to	1
metadata it can	1
metadata requests one	1
metadata server called	1
metadata which is	1
method when replicating	1
methods that are	1
metrics from datanodes	1
michael franklin alon	1
might incorrectly interpret	1
mike cafarella the	1
minimum number of	1
minutes it will	1
minutes to check	1
misleading term that	1
model for largescale	1
model hadoop splits	1
model originally designed	1
modules and submodules	1
modules in hadoop	1
monitoring endtoend performance	1
monitoring hdfs performance	1
monitoring platforms to	1
monitors progress of	1
more conventional supercomputer	1
more efficiently than	1
more than cores	1
more than half	1
moreover there are	1
mostly immutable files	1
mostly written in	1
mounted by the	1
mounted directly with	1
move copies around	1
moved to the	1
moving the data	1
multinode hadoop cluster	1
multiple data centers	1
multiple hadoop clusters	1
multiple hosts and	1
multiple machines it	1
multiple name nodes	1
multiple namespaces served	1
multiple racks this	1
multiple slave nodes	1
multiple worker nodes	1
name node contains	1
name node does	1
name node every	1
name node for	1
name node hdfs	1
name node in	1
name node is	1
name node this	2
name node to	1
name node we	2
name nodes which	1
name of the	1
named it after	1
named it hadoop	1
namenode a misleading	1
namenode and builds	1
namenode and datanode	2
namenode due to	1
namenode goes offline	1
namenode in hadoop	1
namenode is the	1
namenode manually failover	1
namenode plus a	1
namenode regularly connects	1
namenode secondary namenode	1
namenode server to	1
namenode that can	1
namenode when the	1
namenode without having	1
namenodes and the	1
namenodes directory information	1
namenodes memory structures	1
namenodes moreover there	1
namespaces served by	1
naming of products	1
native code in	1
native java api	1
nearby if the	1
nearest to the	1
need for storage	1
need to acquire	1
needed by other	1
needed to start	1
needs to know	1
negotiator which replaced	1
network and prevents	1
network client libraries	1
network if a	1
network of many	1
network switch where	1
network traffic hadoop	1
network traffic on	1
network using a	1
new abstraction for	1
new addition aims	1
new hadoop subproject	1
no consideration of	1
no hdfs file	1
no of blocks	1
no preemption once	1
no rackawareness in	1
node a contains	1
node a data	1
node a to	1
node acts as	1
node and can	1
node and data	1
node and it	1
node as dead	1
node bootstraps the	1
node consists of	1
node contains the	2
node does not	1
node every seconds	1
node for minutes	1
node for the	3
node hdfs consists	1
node in response	1
node is a	1
node is hadoop	1
node is its	1
node it is	1
node sends a	1
node spawns a	1
node stores data	1
node the data	1
node this is	2
node to know	1
node we call	2
node where the	2
node which can	1
node x contains	1
node x would	1
nodes are managed	1
nodes can talk	1
nodes in a	1
nodes in the	3
nodes manipulate the	1
nodes the master	1
nodes these are	1
nodes to process	1
nodes two on	1
nodes when hadoop	1
nodes which solves	1
nominally a single	1
none of which	1
nonposix operations such	1
normally used only	1
not always available	1
not be suitable	1
not fully posixcompliant	1
not having a	1
not receive a	1
not require redundant	1
not restricted to	1
number of available	1
number of companies	1
number of files	1
number of languages	1
number of map	1
number of running	1
number of small	1
number of thirdparty	1
nutch consisted of	1
nutch project but	1
object storage this	1
ocaml the commandline	1
occurrences and should	1
occurs the data	1
october commercial applications	1
october this paper	1
of a hadoop	1
of a job	1
of a rack	1
of a storage	1
of about lines	1
of additional software	1
of an alternative	1
of apache hadoop	2
of applying that	1
of available slots	1
of big data	1
of block replications	1
of blocks locations	1
of code for	2
of companies offer	1
of data and	1
of data high	1
of data it	1
of data locality	1
of data over	1
of data similarly	1
of datanodes although	1
of docker which	1
of failure problem	1
of failure spof	1
of files especially	1
of filesystem actions	1
of gigabytes to	1
of gpu hardware	1
of hadoop adoption	1
of hadoop and	1
of hadoop included	1
of hadoop ship	1
of hadoop to	1
of hadoop was	1
of hadoop yarn	1
of hdfs are	1
of hdfs that	1
of higherend hardware	1
of independent disks	1
of its hadoop	1
of languages eg	1
of locality to	1
of many computers	1
of map slots	1
of metadata it	1
of not having	1
of nutch consisted	1
of october commercial	1
of one jobtracker	1
of only one	1
of opensource software	1
of or alongside	1
of petabytes of	1
of posix compliance	1
of priority has	1
of products and	1
of relationaltabular data	1
of running jobs	1
of service qos	1
of small files	1
of storage and	1
of storage in	1
of supported file	1
of the allocated	1
of the biggest	1
of the checkpoints	1
of the current	1
of the data	3
of the execution	1
of the fair	2
of the file	2
of the following	1
of the fortune	1
of the hadoop	1
of the job	1
of the jobtracker	1
of the mapreduce	2
of the namenodes	1
of the no	1
of the primary	1
of the rack	1
of the total	1
of the underlying	1
of the users	1
of these are	1
of these hardware	1
of thirdparty file	1
of this approach	1
of traffic that	1
of two different	1
of underlying operating	1
of using hdfs	1
of various kinds	1
of which are	2
of work every	1
of work that	1
of work to	1
of xml messages	1
of yarn yet	1
offer commercial implementations	1
officially released by	1
offline in fact	1
often used for	1
omalley was the	1
on a b	1
on a different	1
on a hadoop	1
on a linux	1
on a parallel	1
on application development	1
on clusters of	1
on commodity machines	1
on each node	1
on february yahoo	1
on hosts but	1
on jobcompletion times	1
on large clusters	2
on linux and	1
on mapreduce and	1
on multiple slave	1
on remotely accessible	1
on some other	1
on the actual	1
on the amazon	1
on the apache	1
on the file	2
on the main	1
on the node	1
on the number	1
on the same	2
on the storage	1
on three nodes	1
on top of	2
on which hdfs	1
on x y	1
once a job	1
one advantage of	1
one from google	1
one jobtracker to	1
one name node	2
one of the	1
one on a	1
one single namenode	1
one slot the	1
one tasktracker is	1
only in nonstandard	1
only one name	2
only software officially	1
only to take	1
onsite datacenter as	1
onto a backup	1
oozie and apache	1
open source distributed	1
opensource implementation of	1
opensource software portal	1
opensource software utilities	1
operating system by	1
operating system level	1
operating system there	1
operating systems the	1
operations such as	1
optionally scheduling priorities	1
options are available	1
or alongside hadoop	1
or collection of	1
or distributions of	1
or higher the	1
or mapreduce jobs	1
or reduce jobs	1
or reduce task	1
or reduce tasks	2
or remote directories	1
or specific setup	1
or support for	1
or switch failure	1
or the capacity	1
or times out	1
or via rdparty	1
or worker node	1
or yarnmr and	1
organizations to deploy	1
originally designed for	1
other and in	1
other applications many	1
other data node	1
other details as	1
other file systems	3
other hadoop modules	1
other machines are	1
other name node	1
other projects in	1
other to rebalance	1
other unix systems	1
other vendors and	1
out of nutch	1
out of the	1
out that part	1
outage or switch	1
over http or	1
over the ibm	1
over the network	2
overhead with erasure	1
owen omalley was	1
own cloudiq storage	1
own rackaware file	1
package contains the	1
package which provides	1
packaged code into	1
packages that can	1
paper inspired doug	1
paper spawned another	1
paper that was	1
papers influenced the	1
papers on mapreduce	1
parallel file system	2
parallel processing of	1
parallel this approach	1
parascale file system	1
parascale published the	1
part known as	1
part of the	1
part which is	1
particular name node	1
parts of the	1
pb and later	1
pb of storage	1
pb per day	1
perform is known	1
perform map or	2
performance at scale	1
performance bottlenecks since	1
performance computing cluster	1
performance for data	1
performance including hortonworks	1
performance requires tracking	1
performance some raid	1
perl haskell c	1
permanently into the	1
permits usage of	1
petabytes of storage	1
phoenix apache spark	1
php ruby erlang	1
pig apache hive	1
platform on which	1
platform responsible for	1
platforms and for	1
platforms to track	1
plus a cluster	1
point failure it	1
point for storage	1
point of failure	2
pool is assigned	1
pool pools have	1
pools have to	1
portability across various	1
portability limitations that	1
portable file system	1
posix compliance but	1
posix filesystem differ	1
posixcompliant because the	1
posixcompliant filesystem is	1
possible to have	1
possible with a	1
power outage or	1
preemption once a	1
prevent the tasktracker	1
preventing filesystem corruption	1
prevents unnecessary data	1
price the loss	1
primary namenode and	1
primary namenode goes	1
primary namenode without	1
primary namenodes directory	1
principle of docker	1
priorities to schedule	1
priority has access	1
priority is given	1
problem to a	1
problems involving massive	1
problems single point	1
procedure calls rpc	1
process of applying	1
process of block	1
process the data	1
process to prevent	1
processed faster and	1
processing of big	1
processing of data	1
processing of xml	1
processing on large	2
processing part which	1
processing some of	1
processing the data	3
produced data that	1
production application the	1
production jobs the	1
products and derivative	1
program other projects	1
programming interface api	1
programming language can	1
programming language with	1
programming model for	1
programming model hadoop	1
programming model originally	1
progress of the	1
project but was	1
project can be	1
project hadoop was	1
project has also	1
projects in the	1
prominent use cases	1
protocol specific to	1
provide apis for	1
provide fast response	1
provide location awareness	1
provide shell commands	1
provided by hadoop	1
provides a software	1
provides file system	1
providing very high	1
published in october	2
published the source	1
pushes work to	1
python php ruby	1
qos for production	1
quality of service	1
queries and regular	1
query there are	1
queue a job	1
queue in version	1
queues are allocated	1
queues beyond their	1
rack and one	1
rack data nodes	1
rack power outage	1
rack specifically the	1
rack this reduces	1
rackaware file system	2
rackawareness in this	1
racks this approach	1
rackswitch to reduce	1
raid configurations are	1
raid storage on	1
randomaccess readwrite file	1
range of gigabytes	1
rather than realtime	1
rdparty network client	1
read and write	1
readwrite file system	1
realtime is very	1
realtime system such	1
rebalance data to	1
receive a heartbeat	1
receives code from	1
receives the requests	1
reduce backbone traffic	1
reduce execution from	1
reduce jobs to	1
reduce network traffic	1
reduce parts of	1
reduce slots as	1
reduce task takes	1
reduce tasks on	2
reduces network traffic	1
reduces the amount	1
reduces the impact	1
reduces time spent	1
redundancy across multiple	1
redundancy options are	1
redundant array of	1
refactored out of	1
regular http for	1
regularly connects with	1
relationaltabular data eg	1
released a hadoop	1
released by the	1
released in april	1
reliability by replicating	1
relies on a	1
remote directories these	1
remote procedure calls	1
remotely accessible ftp	1
replaced by the	1
replaced the hdfs	1
replaced the mapreduce	1
replay the entire	1
replicating data for	1
replicating the data	1
replication of data	1
replication value data	1
replications are stored	1
replications on some	1
request the name	1
requests for map	1
requests one advantage	1
require redundant array	1
require that secure	1
requirements for a	1
requires java runtime	1
requires tracking metrics	1
requiring concurrent write	1
rescheduled the tasktracker	1
resides priority is	1
resource allocation to	1
resource manager which	1
resource negotiator which	1
resources are allocated	1
resources in clusters	1
resources to various	1
response gives the	1
response times for	1
responsible for managing	1
responsible for the	1
restart a failed	1
restricted to mapreduce	1
result in some	1
richer user interfaces	1
risk solutions high	1
robert kallman et	1
roughly half a	1
rpc to communicate	1
ruby erlang perl	1
run hadoop against	1
running due to	1
running hadoop over	1
running job crashes	1
runs on a	1
runs on top	1
runs two dmons	1
runtime environment jre	1
s simple storage	1
same rack and	1
same rack this	1
same rackswitch to	1
same way slave	1
sanjay ghemawat mapreduce	1
saves to local	1
scalability problems single	1
scalable and portable	1
scale has become	1
scale to tens	1
schedule jobs from	1
scheduled to perform	1
scheduler described next	1
scheduler has three	1
scheduler is to	1
scheduler or the	1
scheduler such as	1
scheduler supports several	1
scheduler was developed	2
scheduler was refactored	1
schedules map or	1
schedules node a	1
scheduling across nodes	1
scheduling and optionally	1
scheduling of work	1
scheduling priorities to	1
scheduling users applications	1
scripts needed to	1
scripts require that	1
scripts though mapreduce	1
search engine in	1
search query there	1
search webmap is	1
secondary name node	2
secondary namenode a	1
secondary namenode and	1
secondary namenode regularly	1
secondary namenode that	1
seconds and conveys	1
sectorsphere open source	1
secure shell ssh	1
see also hadoop	1
sends a heartbeat	1
sent from the	1
separate java virtual	1
separate namenodes moreover	1
served by separate	1
server called the	1
server can manage	1
server to host	1
serverondemand infrastructure there	1
servers are closest	1
serves up blocks	1
service object storage	1
service qos for	1
services as follows	1
services can communicate	2
services master services	1
servicesdaemonsnodes and bottom	1
set up between	1
several features that	1
several monitoring platforms	1
shell commands and	1
shell scripts though	1
shell ssh be	1
ship with an	1
should be automatically	1
should provide location	1
shutdown scripts require	1
significant impact on	1
similar to other	1
similar to those	1
similarly a standalone	1
simple every tasktracker	1
simple linux utility	1
simple storage service	1
simplified data processing	2
simply using a	1
since the java	1
single master and	1
single namenode in	1
single namenode plus	1
single point failure	1
single point for	1
single point of	2
single task can	1
slave daemons every	1
slave node and	2
slave node for	1
slave or worker	1
slave services can	1
slave services master	1
slot the job	1
slot there is	1
slots as well	1
slots every active	1
slots reduce slots	1
slots such as	1
slow it can	1
slowest task with	1
small file issues	1
small files hdfs	1
small hadoop cluster	1
small jobs and	1
smalltalk and ocaml	1
snapshots of the	2
socalled secondary namenode	1
sockets for communication	1
software foundation has	1
software framework for	1
software officially released	1
software packages that	1
software utilities that	1
solutions high performance	1
solve problems involving	1
solves the single	1
some commercial distributions	1
some consider it	1
some issues in	1
some might incorrectly	1
some native code	1
some of these	1
some other data	1
some other unix	1
some papers influenced	1
some performance bottlenecks	1
some raid configurations	1
somewhat controversial within	1
sons toy elephant	2
sophisticated data mining	1
sort of work	1
source code of	1
source code to	1
source code was	1
source distributed storage	1
spark apache zookeeper	1
spawned another one	1
spawns a separate	1
specific setup expertise	1
specific to hdfs	1
specifically ibm and	1
specifically the network	1
specify the minimum	1
speculative execution enabled	1
spent on application	1
split across multiple	1
split between jobs	1
splits files into	1
spof and bottlenecks	1
sqoop apache oozie	1
ssh be set	1
standalone jobtracker server	1
standard startup and	1
started developing automatic	1
started on the	1
starts the process	1
startup and shutdown	1
stated that only	1
status and information	1
status the job	1
still useful with	1
storage and management	1
storage and processing	2
storage and runs	1
storage blobs wasb	1
storage in june	1
storage management layer	1
storage on hosts	1
storage overhead with	1
storage part known	1
storage service object	1
storage system for	1
storage systems to	1
storage systems understanding	1
storage this is	1
store due to	1
stored and other	1
stored and where	1
stored on three	1
stores all its	1
stores data in	1
stores data on	1
stores large files	1
stores the actual	1
stores without moving	1
storing the data	1
storm flink and	1
streaming to implement	1
strives to allocate	1
striving to keep	1
structure because the	1
structured data google	1
structures thereby preventing	1
submit mapreduce jobs	1
submodules and also	1
subproject in january	1
substantial benefit to	1
such as apache	1
such as append	1
such as lambda	1
such as slots	1
such as small	1
such as the	1
suitable for systems	1
supercomputer architecture that	1
support for hadoop	1
support for nonposix	1
supported file systems	1
supporting a huge	1
supports access from	1
supports several features	1
switch failure if	1
switch where a	1
system and has	1
system and operating	1
system and the	1
system apache hbase	1
system as it	1
system as the	1
system bridges can	1
system bridges have	1
system by simply	1
system driver for	1
system for hadoop	1
system for structured	1
system hadoop can	1
system hdfs a	1
system hdfs and	1
system hdfs is	1
system hdfs the	1
system includes a	1
system index and	1
system is not	1
system level abstractions	1
system load of	1
system metadata which	1
system on linux	1
system paper that	1
system should provide	1
system such as	1
system that can	1
system the jobtracker	1
system the namenode	1
system the source	1
system then saves	1
system there are	1
system this is	2
system this stores	1
system uses tcpip	1
system where computation	1
system with a	1
system written in	1
systems a hadoop	1
systems bundled with	1
systems comes the	1
systems of the	1
systems or mapreduce	1
systems requiring concurrent	1
systems the hdfs	1
systems this advantage	1
systems to accept	1
systems understanding of	1
table and storage	1
tackle this problem	1
take care of	2
take that data	1
take the code	1
take the task	1
takes advantage of	1
takes up one	1
talk to each	1
talk with each	1
talks to the	1
target goals of	1
targeted at clusters	1
task can be	1
task from the	1
task takes up	1
task tracker it	1
task tracker namenode	1
task tracker the	1
task tracker will	1
task trackers with	1
task with speculative	1
tasks on a	1
tasks on x	1
tasks the resource	1
tasktracker fails or	1
tasktracker has a	1
tasktracker is very	1
tasktracker itself from	1
tasktracker nodes in	1
tasktracker on each	1
tasktracker status and	1
tasktracker the mapreduce	1
tasktracker though it	1
tasktracker to the	1
tasktrackers is very	1
tcpip sockets for	1
technologies inc announced	1
tens of petabytes	1
terabytes across multiple	1
term compatible are	1
term hadoop is	1
term that some	1
than cores and	1
than half of	1
than it would	1
than realtime is	1
that allows distributions	1
that are being	1
that are exclusive	1
that are similar	2
that are uncategorized	1
that can be	2
that can generate	1
that code on	1
that data node	1
that evolve based	1
that facilitate using	1
that goes over	1
that hadoop decreases	1
that hadoopspecific file	1
that hardware failures	1
that is batchoriented	1
that it is	1
that on the	1
that only software	1
that part of	1
that relies on	1
that result in	1
that runs on	1
that secure shell	1
that some might	1
that stores data	1
that supports access	1
that the clusters	1
that the data	1
that they had	1
that uses json	1
that was factored	1
that was published	1
that was used	1
that year they	1
the ability to	1
the actual data	1
the actual node	1
the addition of	1
the allocated machine	1
the allocation of	1
the amazon elastic	1
the amount of	1
the apache hadoop	1
the apache hive	1
the apache mahout	1
the apache nutch	1
the apache software	1
the application master	1
the authors highlight	1
the availability of	1
the base apache	1
the biggest changes	1
the biggest difference	1
the birth and	1
the blocks this	1
the capacity scheduler	3
the checkpoint node	1
the checkpoints of	1
the client job	1
the client to	1
the cloud allows	1
the cloud the	1
the cluster striving	1
the cluster which	1
the clusters perform	1
the code and	1
the commandline interface	1
the common useit	1
the core of	1
the current system	1
the data across	1
the data and	2
the data as	1
the data had	1
the data hdfs	1
the data in	1
the data information	1
the data is	2
the data job	1
the data like	1
the data location	1
the data name	1
the data permanently	1
the data resides	1
the data they	1
the data was	1
the data will	1
the data with	1
the dataset to	1
the default replication	1
the default specifically	1
the details of	1
the ecosystem or	1
the end when	1
the entire journal	1
the entire mapreduce	1
the fair scheduler	5
the file is	1
the file system	4
the file systems	2
the file the	1
the files manage	1
the filesystemspecific equivalents	1
the first committer	1
the first version	1
the following modules	1
the fortune companies	1
the genesis of	1
the goal of	1
the google file	1
the hadoop common	2
the hadoop developer	1
the hadoop distributed	2
the hadoop distribution	1
the hadoop ecosystem	1
the hadoop framework	2
the hadoop project	1
the hbase database	1
the hdfs design	1
the hdfs file	3
the hdfsui web	1
the ibm general	1
the impact of	1
the index calculations	1
the initial code	1
the java archive	1
the java implementation	1
the java programming	1
the job is	1
the job scheduler	1
the job tracker	8
the jobtracker every	1
the jobtracker knows	1
the jobtracker pushes	1
the jobtracker while	1
the largest hadoop	1
the linux image	1
the list includes	1
the list of	1
the location of	1
the log to	1
the loss of	1
the main backbone	1
the main metadata	1
the map and	1
the mapreduce engine	3
the mapreduce framework	1
the mapreduce programming	2
the master node	1
the meta data	2
the minimum number	1
the modules in	1
the name node	5
the name of	1
the namenode due	1
the namenode is	1
the namenode manually	1
the namenode secondary	1
the namenodes memory	1
the naming of	1
the native java	1
the need for	1
the need to	1
the network and	1
the network switch	1
the network using	1
the new hadoop	1
the no of	1
the node where	1
the number of	1
the opensource community	1
the parascale file	1
the platform on	1
the primary namenode	2
the primary namenodes	1
the process of	2
the processing the	3
the project has	1
the queues resources	1
the rack specifically	1
the range of	1
the replication of	1
the replications are	1
the requests for	1
the requirements for	1
the resource manager	1
the running job	1
the same rack	2
the same rackswitch	1
the same way	1
the secondary namenode	1
the single point	2
the slave node	2
the slowest task	1
the source code	3
the standard startup	1
the storage systems	1
the system then	1
the target goals	1
the task from	1
the tasktracker itself	1
the tasktracker on	1
the tasktracker to	1
the term compatible	1
the term hadoop	1
the thrift api	1
the time named	1
the total resource	1
the tracker nearest	1
the tradeoff of	1
the underlying operating	3
the users program	1
the whole data	1
the work as	1
the work cannot	1
the world with	1
the worlds largest	1
the yahoo search	2
their total capacity	1
them across nodes	1
them for scheduling	1
then saves to	1
then to edit	1
then transfers packaged	1
theoretically does not	1
theory be used	1
there are containers	1
there are currently	1
there are important	1
there are multiple	1
there are some	1
there is no	3
there is one	1
thereby preventing filesystem	1
these are normally	1
these are slave	1
these checkpointed images	1
these hardware failures	1
they announced that	1
they announced the	1
they claimed was	1
they had the	1
they have access	1
thirdparty file system	1
this advantage is	1
this allows the	1
this approach are	1
this approach reduces	1
this approach takes	1
this can have	1
this comes at	1
this file system	1
this information to	1
this is also	2
this is an	1
this is designed	1
this is only	1
this is targeted	1
this method when	1
this paper inspired	1
this paper spawned	1
this problem to	1
this reduces network	1
this reduces the	1
this stores all	1
this way when	1
those of the	1
though it is	1
though mapreduce java	1
three are master	1
three basic concepts	1
three nodes two	1
thrift api generates	1
through a dedicated	1
through contributions that	1
through the native	1
throughput and support	1
time named it	1
time spent on	1
times as demonstrated	1
times for small	1
times out that	1
to a certain	1
to accept all	1
to access data	1
to acquire hardware	1
to add to	1
to allocate resources	1
to applications the	1
to available tasktracker	1
to be particular	1
to be processed	1
to check its	1
to communicate with	1
to complement a	1
to create an	1
to dataspaces a	1
to deploy hadoop	1
to develop an	1
to each other	1
to edit the	1
to evolve through	1
to execute code	1
to execute deep	1
to have dataonly	1
to hdfs letting	1
to hdfs the	1
to host the	1
to implement the	1
to include the	1
to increase inputoutput	1
to instead be	1
to its cofounders	1
to its criticality	1
to its lack	1
to its widespread	1
to job tracker	1
to keep the	2
to know about	1
to know which	1
to local or	1
to mapreduce jobs	1
to move copies	1
to nodes in	1
to other file	1
to pb and	1
to perform map	2
to prevent the	1
to process the	1
to provide apis	1
to provide fast	1
to queues beyond	1
to read and	1
to rebalance data	1
to reduce backbone	1
to reduce network	1
to replay the	1
to restart a	1
to run hadoop	1
to scale to	1
to schedule jobs	1
to solve problems	1
to specify the	1
to start hadoop	1
to tackle this	1
to take care	1
to task trackers	1
to tasktrackers is	1
to tens of	1
to terabytes across	1
to the data	3
to the hadoop	1
to the jobtracker	1
to the name	2
to the new	1
to the opensource	1
to the platform	1
to the project	1
to the queues	1
to the tracker	1
to this allows	1
to those of	1
to track hdfs	1
to use an	1
to various applications	1
to which client	1
top of or	1
top of the	1
top three are	1
total resource capacity	1
towards the end	1
toy elephant the	1
track hdfs performance	1
track the files	1
tracker allocates work	1
tracker and also	1
tracker and it	1
tracker and task	1
tracker and tasktracker	1
tracker basically job	1
tracker it is	1
tracker namenode and	1
tracker nearest to	1
tracker receives the	1
tracker schedules map	1
tracker schedules node	1
tracker talks to	1
tracker task tracker	2
tracker the job	1
tracker will be	1
tracker will request	1
tracker will take	1
trackers with an	1
tracking and resource	1
tracking metrics from	1
tradeoff of not	1
traditional onsite datacenter	1
traffic hadoop needs	1
traffic hdfs uses	1
traffic on the	1
traffic that goes	1
transaction processing system	1
transfer when hadoop	1
transfers packaged code	1
two are slave	1
two different tasks	1
two dmons which	1
two on the	1
typically in the	1
uncategorized go into	1
under development at	1
underlying operating system	2
underlying operating systems	2
understanding of the	1
unnecessary data transfer	1
up between nodes	1
up blocks of	1
up one slot	1
up waiting for	1
uptodate directory structure	1
url however this	1
usage of gpu	1
use an alternate	1
use features that	1
use on clusters	1
use remote procedure	1
use this information	1
use with its	1
used for any	1
used for both	1
used for other	1
used for storing	1
used for the	1
used in every	1
used only in	1
used to complement	1
used to restart	1
used with an	1
used with hadoop	1
used with other	1
useful in the	1
useful with the	1
useit has also	1
users program other	1
userspace fuse virtual	1
uses fifo scheduling	1
uses json for	1
uses tcpip sockets	1
uses this method	1
using a block	1
using a file	1
using a network	1
using hdfs is	1
using the mapreduce	1
using them for	1
utilities needed by	1
utilities that facilitate	1
utilities written as	1
utility for resource	1
value data is	1
variety of underlying	1
various applications effectively	1
various hardware platforms	1
vendors and the	1
version available to	1
version of hadoop	1
version the job	1
very dataintensive and	1
very high aggregate	1
very simple every	1
very slow it	1
very substantial benefit	1
via highspeed networking	1
via rdparty network	1
viewed from a	1
virtual file system	1
virtual machine jvm	1
waiting for the	1
warehouse system hadoop	1
was designed for	1
was developed by	2
was factored out	1
was growing by	1
was moved to	1
was published in	2
was refactored out	1
was released in	1
was the first	1
was the google	1
was the worlds	1
was used in	1
was working at	1
wasb file system	1
way slave services	1
way when name	1
we call it	2
we have only	1
web application over	1
web crawling andor	1
web search query	1
webmap is a	1
well as a	1
well as in	1
were added to	1
were inspired by	1
what data node	1
what they claimed	1
when everything can	1
when hadoop is	1
when hadoop mapreduce	1
when name node	1
when replicating data	1
when the primary	1
where a worker	1
where computation and	1
where nodes manipulate	1
where the data	2
where the replications	1
which are currently	1
which are under	1
which can track	1
which client applications	1
which consists of	1
which does job	1
which hdfs is	1
which is a	2
which is in	1
which is responsible	1
which is the	1
which monitors progress	1
which node contains	1
which other machines	1
which provides file	1
which reduces time	1
which replaced the	2
which servers are	1
which solves the	1
which take care	1
which the system	1
while adding the	1
while there is	1
who was working	1
whole data in	1
widespread integration into	1
widespread more than	1
will be useful	1
will remain available	1
will request the	1
will take that	1
will take the	2
windows azure storage	1
with a filesystem	1
with a full	1
with a fundamental	1
with a high	1
with a rackaware	1
with a variety	1
with an alternate	1
with an alternative	1
with an available	1
with an awareness	1
with any distributed	1
with apache hadoop	1
with dataintensive jobs	1
with each other	4
with erasure coding	1
with hadoop streaming	1
with its own	1
with more than	1
with other file	1
with pb of	1
with some native	1
with speculative execution	1
with the client	1
with the default	1
with the primary	1
within a queue	1
within the cluster	1
within the hadoop	1
without having to	1
without moving the	1
without the need	1
work as close	1
work cannot be	1
work every hadoopcompatible	1
work queue in	1
work that is	1
work that the	1
work to available	1
work to tasktrackers	1
work to the	1
worker node acts	1
worker node is	1
worker nodes the	1
worker nodes these	1
working at yahoo	1
working in principle	1
works directly with	1
works from other	1
world with pb	1
worlds largest hadoop	1
would be in	1
would be scheduled	1
write these are	1
written as shell	1
written in java	1
written in the	1
written none of	1
x contains data	1
x would be	1
x y z	2
y z the	1
y z this	1
yahoo and no	1
yahoo at the	1
yahoo inc launched	1
yahoo made the	1
yahoo search engine	1
yahoo search webmap	1
yahoo the capacity	1
yahoo web search	1
yarn introduced in	1
yarn strives to	1
yarn yet another	1
yarnmr and the	1
year they announced	1
yet another resource	1
z the job	1
z this reduces	1
zookeeper cloudera impala	1
